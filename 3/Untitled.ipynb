{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start = 2008q3\n",
      "end   = 2009q4\n",
      "2009q2\n",
      "9856 517\n",
      "university mean ratio: 1.05456452839\n",
      "non-university mean ratio: 1.07519622093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State       RegionName     \n",
       "Alabama     Montevallo         0.950479\n",
       "            Tuscaloosa         1.017770\n",
       "Alaska      Fairbanks          1.127970\n",
       "Arizona     Flagstaff          1.084557\n",
       "            Tempe              1.125462\n",
       "            Tucson             1.098763\n",
       "Arkansas    Conway             0.996482\n",
       "            Fayetteville       1.063659\n",
       "California  Arcata             1.113252\n",
       "            Berkeley           1.067911\n",
       "            Chico              1.075782\n",
       "            Claremont          1.097610\n",
       "            Cotati             1.178497\n",
       "            Davis              0.973927\n",
       "            Irvine             1.097770\n",
       "            Merced             1.408205\n",
       "            Orange             1.221979\n",
       "            Palo Alto          1.080125\n",
       "            Pomona             1.498671\n",
       "            Redlands           1.108343\n",
       "            Riverside          1.453341\n",
       "            Sacramento         1.252753\n",
       "            San Diego          1.133248\n",
       "            San Luis Obispo    1.045885\n",
       "            Santa Barbara      1.108410\n",
       "            Santa Cruz         1.045023\n",
       "            Turlock            1.296216\n",
       "            Whittier           1.201855\n",
       "Colorado    Boulder            1.001500\n",
       "            Durango            1.057624\n",
       "                                 ...   \n",
       "Texas       Canyon             0.943013\n",
       "            College Station    0.964469\n",
       "            Commerce           0.997994\n",
       "            Dallas             1.097685\n",
       "            Denton             1.054040\n",
       "            Georgetown         1.040208\n",
       "            Keene              1.017727\n",
       "            Lubbock            0.894820\n",
       "            Stephenville       0.786841\n",
       "Utah        Orem               1.045368\n",
       "            Provo              0.966295\n",
       "            Salt Lake City     0.922925\n",
       "Vermont     Burlington         0.933607\n",
       "            Castleton          0.981829\n",
       "Virginia    Charlottesville    1.050154\n",
       "            Chesapeake         1.062826\n",
       "            Fredericksburg     1.100715\n",
       "            Williamsburg       1.052569\n",
       "            Wise               0.942538\n",
       "Washington  Bellingham         1.035169\n",
       "            Cheney             1.069847\n",
       "            Ellensburg         1.023996\n",
       "Wisconsin   Appleton           1.045403\n",
       "            Eau Claire         1.017889\n",
       "            Green Bay          1.032079\n",
       "            La Crosse          1.058070\n",
       "            Madison            1.015845\n",
       "            Milwaukee          1.124215\n",
       "            Oshkosh            1.088834\n",
       "            Waukesha           1.023734\n",
       "Name: price_ratio, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Dec 15 18:25:42 2016\n",
    "@author: bwaldie\n",
    "\"\"\"\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "#file base path\n",
    "file_path = (\"\")\n",
    "\n",
    "# Use this dictionary to map state names to two letter acronyms\n",
    "states = {'OH': 'Ohio', 'KY': 'Kentucky', 'AS': 'American Samoa', 'NV': 'Nevada', 'WY': 'Wyoming', 'NA': 'National', 'AL': 'Alabama', 'MD': 'Maryland', 'AK': 'Alaska', 'UT': 'Utah', 'OR': 'Oregon', 'MT': 'Montana', 'IL': 'Illinois', 'TN': 'Tennessee', 'DC': 'District of Columbia', 'VT': 'Vermont', 'ID': 'Idaho', 'AR': 'Arkansas', 'ME': 'Maine', 'WA': 'Washington', 'HI': 'Hawaii', 'WI': 'Wisconsin', 'MI': 'Michigan', 'IN': 'Indiana', 'NJ': 'New Jersey', 'AZ': 'Arizona', 'GU': 'Guam', 'MS': 'Mississippi', 'PR': 'Puerto Rico', 'NC': 'North Carolina', 'TX': 'Texas', 'SD': 'South Dakota', 'MP': 'Northern Mariana Islands', 'IA': 'Iowa', 'MO': 'Missouri', 'CT': 'Connecticut', 'WV': 'West Virginia', 'SC': 'South Carolina', 'LA': 'Louisiana', 'KS': 'Kansas', 'NY': 'New York', 'NE': 'Nebraska', 'OK': 'Oklahoma', 'FL': 'Florida', 'CA': 'California', 'CO': 'Colorado', 'PA': 'Pennsylvania', 'DE': 'Delaware', 'NM': 'New Mexico', 'RI': 'Rhode Island', 'MN': 'Minnesota', 'VI': 'Virgin Islands', 'NH': 'New Hampshire', 'MA': 'Massachusetts', 'GA': 'Georgia', 'ND': 'North Dakota', 'VA': 'Virginia'}\n",
    "\n",
    "def read_university_towns():\n",
    "    u_states = list()\n",
    "    u_regions = list()\n",
    "    university_towns = pd.DataFrame()\n",
    "    with open(file_path+\"university_towns.txt\",\"r\") as fh:\n",
    "        for line in fh:\n",
    "            # print(line)\n",
    "            m1 = re.search(r'(.*?)\\s*\\[edit\\]', line)\n",
    "            m2 = re.search(r'^(.*?)\\s+\\(', line)\n",
    "            if (m1 != None):\n",
    "                # print(m1.group(1))\n",
    "                state = m1.group(1)\n",
    "            elif (m2 != None):\n",
    "                # print(state, \" \",m2.group(1))\n",
    "                u_states.append(state)\n",
    "                u_regions.append(m2.group(1))\n",
    "            else:\n",
    "                u_states.append(state)\n",
    "                u_regions.append(line.strip('\\n'))\n",
    "                #print(line)\n",
    "    university_towns = pd.DataFrame({ 'State': u_states, 'RegionName' : u_regions})\n",
    "    university_towns = university_towns[['State','RegionName']]\n",
    "    return university_towns\n",
    "\n",
    "def get_list_of_university_towns():\n",
    "    '''Returns a DataFrame of towns and the states they are in from the \n",
    "    university_towns.txt list. The format of the DataFrame should be:\n",
    "    DataFrame( [ [\"Michigan\", \"Ann Arbor\"], [\"Michigan\", \"Yipsilanti\"] ], \n",
    "    columns=[\"State\", \"RegionName\"]  )\n",
    "    \n",
    "    The following cleaning needs to be done:\n",
    "    1. For \"State\", removing characters from \"[\" to the end.\n",
    "    2. For \"RegionName\", when applicable, removing every character from \" (\" to the end.\n",
    "    3. Depending on how you read the data, you may need to remove newline character '\\n'. '''\n",
    "    \n",
    "    university_towns = pd.DataFrame()\n",
    "    university_towns = read_university_towns()\n",
    "    return university_towns\n",
    "    \n",
    "def read_gdp_xls():\n",
    "    GDP = pd.DataFrame()\n",
    "    GDP = pd.read_excel(file_path+\"gdplev.xls\",skiprows=7)\n",
    "    GDP = GDP[['Unnamed: 4', 'Unnamed: 6']]\n",
    "    GDP = GDP.rename(columns={'Unnamed: 4' : 'Quarter', 'Unnamed: 6':'GDP'})\n",
    "    GDP['delta']=GDP['GDP'].diff()\n",
    "    GDP['delta_shifted-1'] = GDP['delta'].shift(-1)\n",
    "    return GDP\n",
    "\n",
    "dfGDP = read_gdp_xls()\n",
    "# print(dfGDP)\n",
    "\n",
    "def get_recession_start():\n",
    "    '''Returns the year and quarter of the recession start time as a \n",
    "    string value in a format such as 2005q3'''\n",
    "    mask_start_GDP = ((dfGDP['delta']<0) & (dfGDP['delta_shifted-1']<0) & (dfGDP['Quarter'] >= '2000q1'))\n",
    "    df_start_GDP = dfGDP[mask_start_GDP]\n",
    "    return df_start_GDP['Quarter'].iloc[0]\n",
    "\n",
    "recession_start = get_recession_start()\n",
    "print(\"start = \" + recession_start)\n",
    "\n",
    "def get_recession_end():\n",
    "    '''Returns the year and quarter of the recession end time as a \n",
    "    string value in a format such as 2005q3'''\n",
    "    mask_stop_GDP = ((dfGDP['delta']>=0) & (dfGDP['delta_shifted-1']>=0) & (dfGDP['Quarter'] >= recession_start))\n",
    "    df_stop_GDP = dfGDP[mask_stop_GDP]\n",
    "    return df_stop_GDP['Quarter'].iloc[1]\n",
    "\n",
    "recession_end = get_recession_end()\n",
    "print(\"end   = \" + recession_end)\n",
    "\n",
    "def get_recession_bottom():\n",
    "    '''Returns the year and quarter of the recession bottom time as a \n",
    "    string value in a format such as 2005q3'''\n",
    "    mask_bottom_GDP = ((dfGDP['Quarter'] >= recession_start) & (dfGDP['Quarter'] < recession_end))\n",
    "    df_bottom_GDP = dfGDP[mask_bottom_GDP]\n",
    "    df_bottom_GDP = df_bottom_GDP.set_index('Quarter')\n",
    "    bottom_idx =  df_bottom_GDP['GDP'].idxmin()\n",
    "    return bottom_idx\n",
    "\n",
    "print(get_recession_bottom())\n",
    "\n",
    "def read_zillow_data():\n",
    "    df_zillow = pd.read_csv(file_path+\"City_Zhvi_AllHomes.csv\")\n",
    "    df_zillow['State'].replace(states, inplace=True)\n",
    "    df_zillow.drop(df_zillow.columns[6:51], axis=1, inplace=True)\n",
    "    df_zillow['2016-09'] = df_zillow[['2016-07', '2016-08']].mean(axis=1)\n",
    "    return df_zillow\n",
    "\n",
    "def convert_housing_data_to_quarters():\n",
    "    '''Converts the housing data to quarters and returns it as mean \n",
    "    values in a dataframe. This dataframe should be a dataframe with\n",
    "    columns for 2000q1 through 2016q3, and should have a multi-index\n",
    "    in the shape of [\"State\",\"RegionName\"].\n",
    "    \n",
    "    Note: Quarters are defined in the assignment description, they are\n",
    "    not arbitrary three month periods.\n",
    "    \n",
    "    The resulting dataframe should have 67 columns, and 10,730 rows.\n",
    "    \n",
    "    df.columns = pd.to_datetime(df.columns)\n",
    "    res = df.resample('Q', axis=1).mean()\n",
    "    res = res.rename(columns=lambda col: '{}q{}'.format(col.year, col.quarter))\n",
    "    pd.concat([df, res], axis=1)\n",
    "    '''\n",
    "    df_zillow = read_zillow_data()\n",
    "    df_zillow.set_index(['State','RegionName'], inplace=True)\n",
    "    df_zillow.drop(df_zillow[['RegionID','Metro','CountyName','SizeRank']], axis=1, inplace=True)\n",
    "    # df_zillow.columns = pd.to_datetime(df_zillow.columns) # will work too, but the below covers cases where\n",
    "    # there are NON-date column headings\n",
    "    df_zillow.rename(columns = lambda col: pd.to_datetime(col, errors='ignore', format=\"%Y-%m\"), inplace=True)\n",
    "    df_zillow = df_zillow.resample('Q', axis=1).mean()\n",
    "    df_zillow.rename(columns = lambda col: '{}q{}'.format(col.year, col.quarter), inplace=True)\n",
    "    return df_zillow\n",
    "\n",
    "\n",
    "def add_price_ratio_column():\n",
    "    ts = pd.Timestamp(pd.Timestamp(get_recession_start())-pd.DateOffset(months=3))\n",
    "    quarter_before_recession = str(ts.to_period('Q')).lower()\n",
    "    recession_bottom = get_recession_bottom()\n",
    "    df_z = convert_housing_data_to_quarters()\n",
    "    df_z.dropna(axis=0, how='all', subset=[recession_bottom, quarter_before_recession], inplace=True)\n",
    "    df_z['price_ratio'] = df_z[quarter_before_recession]/df_z[recession_bottom]\n",
    "    df_z.dropna(axis=0, how='all', subset=['price_ratio'], inplace=True)\n",
    "    return df_z[['price_ratio']]\n",
    "\n",
    "def create_data_sets():\n",
    "    df_full = add_price_ratio_column()\n",
    "    df_university_towns = get_list_of_university_towns()\n",
    "    df_university_towns.set_index(['State','RegionName'],inplace=True)\n",
    "    print(len(df_full), len(df_university_towns))\n",
    "\n",
    "    df_univ_yes = pd.merge(df_university_towns, df_full, how='inner', left_index=True, right_index=True, indicator=True)\n",
    "    df_univ_no  = pd.merge(df_university_towns, df_full, how='outer', left_index=True, right_index=True, indicator=True)\n",
    "    df_univ_yes = df_univ_yes[(df_univ_yes['_merge'] == 'both')]\n",
    "    df_univ_no  = df_univ_no[(df_univ_no['_merge'] == 'right_only')]\n",
    "\n",
    "    return (df_univ_yes['price_ratio'], df_univ_no['price_ratio'])\n",
    "\n",
    "\n",
    "def run_ttest():\n",
    "    '''First creates new data showing the decline or growth of housing prices\n",
    "    between the recession start and the recession bottom. Then runs a ttest\n",
    "    comparing the university town values to the non-university towns values, \n",
    "    return whether the alternative hypothesis (that the two groups are the same)\n",
    "    is true or not as well as the p-value of the confidence. \n",
    "    \n",
    "    Return the tuple (different, p, better) where different=True if the t-test is\n",
    "    True at a p<0.01 (we reject the null hypothesis), or different=False if \n",
    "    otherwise (we cannot reject the null hypothesis). The variable p should\n",
    "    be equal to the exact p value returned from scipy.stats.ttest_ind(). The\n",
    "    value for better should be either \"university town\" or \"non-university town\"\n",
    "    depending on which has a lower mean price ratio (which is equivilent to a\n",
    "    reduced market loss).'''\n",
    "    \n",
    "    df_yes, df_no = create_data_sets()\n",
    "    # print(len(df_yes), len(df_no))\n",
    "    # print(df_yes.describe())\n",
    "    # print(df_no.describe())\n",
    "    \n",
    "    ans = ttest_ind(df_no, df_yes, axis=0, equal_var=False, nan_policy='omit')\n",
    "    # print(ans)\n",
    "    p = ans[1]\n",
    "    \n",
    "    if (p > 0.01):\n",
    "        different = False\n",
    "    else:\n",
    "        different = True\n",
    "    \n",
    "    print(\"university mean ratio: \" + str(df_yes.mean(axis=0)))\n",
    "    print(\"non-university mean ratio: \" + str(df_no.mean(axis=0)))\n",
    "    \n",
    "    if (df_yes.mean(axis=0) > df_no.mean(axis=0)):\n",
    "        better = 'non-university town'\n",
    "    else:\n",
    "        better = 'university town'\n",
    "        \n",
    "    return df_yes#(different, p, better)\n",
    "\n",
    "run_ttest()\n",
    "#print(run_ttest())\n",
    "#print('(True, 0.00041476360323563295, \\'university town\\')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
